{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='appendix'></a>\n",
    "## Appendix\n",
    "### Weights and Flops estimation\n",
    "The following formulas give estimations of number of weights and operations for training fully-connected and convolutional neural networks. Training consists of the forward propagation, backpropagation of error and gradient computation. Lest consider fully-connected networks:\n",
    "  - Weights: intputs times outputs for each layer plus bias: w = Sum(mn+n)\n",
    "  - Training \n",
    "    - Forward: vector-matrix multiplication for each layer, bias addition and activation function, ~2mn=2w\n",
    "    - Backward: vector-matrix and elementwise vector multiplications, ~2nm = 2w\n",
    "    - Gradient computation: column-vector row-vector products, ~nm = w\n",
    "    - Gradient update and total: 5w + 1w = 6w\n",
    "    - Counting \"multiply-add\" operations: 3w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weightsAndFlops(layers):\n",
    "    w = 0\n",
    "    for i in range(1, len(layers)):\n",
    "        w = w + layers[i - 1] * layers[i] + layers[i]\n",
    "    return (w, 3 * w)\n",
    "assert(weightsAndFlops([784, 10])[0] == 7850)\n",
    "# MSFT model [3]\n",
    "assert(round(weightsAndFlops([39 * 11, 2048, 2048, 2048, 2048, 2048, 2048, 2048, 9304])[0] / 1E5, 0) * 1E5 == 45.1E6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks:\n",
    "  - Weights: (feature maps size plus bias) times depth and number of maps\n",
    "  - Training\n",
    "     - Forward: dot product of input and weights, summation of the result and bias addition, for each feature map and output neuron\n",
    "     - Backward: similar\n",
    "     - Gradient computation: similar\n",
    "     - Total in \"multiply-add\" operations: 3w\n",
    "     \n",
    "Bias adds a lot more weights to convolutional networks, so in recent papers it is not used given that data is normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def computeConvOutput(data, layer):\n",
    "    n = layer['count']\n",
    "    k = layer['size']\n",
    "    b = layer['border']\n",
    "    s = layer['stride']\n",
    "    c = (data[0] - k + b) // s + 1   \n",
    "    return [c, c, n]\n",
    "def computePoolOutput(data, layer):\n",
    "    n = data[2]\n",
    "    k = layer['size']\n",
    "    b = layer['border']\n",
    "    s = layer['stride']\n",
    "    c = (data[0] - k + b) // s + 1   \n",
    "    return [c, c, n]\n",
    "def cnnWeightsAndFlops(inputLayer, layers, useBias):\n",
    "    w = 0.0\n",
    "    f = 0.0\n",
    "    inputData = inputLayer\n",
    "    for i in range(0, len(layers)):\n",
    "        layer = layers[i]\n",
    "        if (layer['type'] == 'conv'):\n",
    "            n = layer['count']\n",
    "            k = layer['size']\n",
    "            d = layer['depth']\n",
    "            inputData = computeConvOutput(inputData, layer)\n",
    "            c = inputData[0]\n",
    "            bw = 0\n",
    "            if (useBias): \n",
    "                bw = c * c\n",
    "            w = w + n * (k * k * d + bw)\n",
    "            f = f + n * (k * k * d * c * c)\n",
    "        if (layer['type'] == 'pool'):\n",
    "            n = inputData[2]\n",
    "            inputData = computePoolOutput(inputData, layer)\n",
    "        if (layer['type'] == 'full'):\n",
    "            k = layer['size']\n",
    "            inputFlat = np.product(inputData)\n",
    "            bw = 0\n",
    "            if (useBias):\n",
    "                bw = k\n",
    "            w = w + k * inputFlat + bw\n",
    "            f = f + k * inputFlat\n",
    "            inputData = [k]\n",
    "    return (w, 3 * f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# LeCun et al. 1982 (1068 + 2592 + 5790 + 310 == 9760 weights)\n",
    "assert cnnWeightsAndFlops([16, 16, 1], [{'type' : 'conv', 'count' : 12, 'size' : 5, 'depth' : 1, 'stride': 2, 'border' : 3}], True)[0] == 1068\n",
    "assert cnnWeightsAndFlops([16, 16, 1], [{'type' : 'conv', 'count' : 12, 'size' : 5, 'depth' : 1, 'stride': 2, 'border' : 3},\n",
    "                         {'type' : 'conv', 'count' : 12, 'size' : 5, 'depth' : 8, 'stride': 2, 'border' : 3},\n",
    "                         {'type' : 'full', 'size' : 30},\n",
    "                         {'type' : 'full', 'size' : 10}], True)[0] == 9760\n",
    "# Krizhevsky et al. 2012, AlexNet, see also https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/deploy.prototxt\n",
    "assert(computeConvOutput([227, 227, 3], {'type' : 'conv', 'count' : 96, 'size' : 11, 'depth' : 3, 'stride': 4, 'border' : 0}) == [55, 55, 96])\n",
    "assert(computePoolOutput([55, 55, 96], {'type' : 'pool', 'size' : 3, 'stride': 2, 'border' : 0}) == [27, 27, 96])\n",
    "assert(computeConvOutput([27, 27, 256], {'type' : 'conv', 'count' : 256, 'size' : 5, 'depth' : 48, 'stride': 1, 'border' : 4}) == [27, 27, 256])\n",
    "assert(computePoolOutput([27, 27, 256], {'type' : 'pool', 'size' : 3, 'stride': 2, 'border' : 0}) == [13, 13, 256])\n",
    "assert(computeConvOutput([13, 13, 256], {'type' : 'conv', 'count' : 384, 'size' : 3, 'depth' : 256, 'stride': 1, 'border' : 2}) == [13, 13, 384])\n",
    "assert(computeConvOutput([13, 13, 384], {'type' : 'conv', 'count' : 384, 'size' : 3, 'depth' : 192, 'stride': 1, 'border' : 2}) == [13, 13, 384])\n",
    "assert(computeConvOutput([13, 13, 256], {'type' : 'conv', 'count' : 256, 'size' : 3, 'depth' : 192, 'stride': 1, 'border' : 2}) == [13, 13, 256])\n",
    "assert(computePoolOutput([13, 13, 256], {'type' : 'pool', 'size' : 3, 'stride': 2, 'border' : 0}) == [6, 6, 256])\n",
    "#example AlexNet from http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf\n",
    "alexWeights, alexFlops = cnnWeightsAndFlops([227, 227, 3], [\n",
    "                           {'type' : 'conv', 'count' : 96, 'size' : 11, 'depth' : 3, 'stride': 4, 'border' : 0},\n",
    "                           {'type' : 'pool', 'size' : 3, 'stride': 2, 'border' : 0},\n",
    "                           {'type' : 'conv', 'count' : 256, 'size' : 5, 'depth' : 48, 'stride': 1, 'border' : 4},\n",
    "                           {'type' : 'pool', 'size' : 3, 'stride': 2, 'border' : 0},\n",
    "                           {'type' : 'conv', 'count' : 384, 'size' : 3, 'depth' : 256, 'stride': 1, 'border' : 2},\n",
    "                           {'type' : 'conv', 'count' : 384, 'size' : 3, 'depth' : 384, 'stride': 1, 'border' : 2}, # depth is 192 in paper\n",
    "                           {'type' : 'conv', 'count' : 256, 'size' : 3, 'depth' : 192, 'stride': 1, 'border' : 2},\n",
    "                           {'type' : 'pool', 'size' : 3, 'stride': 2, 'border' : 0}, \n",
    "                           {'type' : 'full', 'size' : 4096},\n",
    "                           {'type' : 'full', 'size' : 4096},\n",
    "                           {'type' : 'full', 'size' : 1000}\n",
    "                          ], False)\n",
    "# weights ~ 60M, forward pass ~ 832M Flops\n",
    "assert(round(alexWeights / 1E7) * 1E7 == 60E6)\n",
    "assert(round(alexFlops / 3E8) * 1E8 == 8E8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "  1. Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. \"Imagenet classification with deep convolutional neural networks.\" Advances in neural information processing systems. 2012. (slides: http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf)\n",
    "  2. Szegedy, Christian, et al. \"Rethinking the Inception Architecture for Computer Vision.\" arXiv preprint arXiv:1512.00567 (2015).\n",
    "  3. Seide, Frank, Gang Li, and Dong Yu. \"Conversational Speech Transcription Using Context-Dependent Deep Neural Networks.\" Interspeech. 2011.\n",
    "  4. Iandola, Forrest N., et al. \"FireCaffe: near-linear acceleration of deep neural network training on compute clusters.\" arXiv preprint arXiv:1511.00175 (2015).\n",
    "  5. Chen, Jianmin, et al. \"Revisiting Distributed Synchronous SGD.\" arXiv preprint arXiv:1604.00981 (2016).\n",
    "  6. Chapelle, Olivier, Eren Manavoglu, and Romer Rosales. \"Simple and scalable response prediction for display advertising.\" ACM Transactions on Intelligent Systems and Technology (TIST) 5.4 (2015): 61.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
